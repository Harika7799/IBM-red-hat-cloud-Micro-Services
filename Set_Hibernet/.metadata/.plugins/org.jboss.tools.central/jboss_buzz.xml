<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Introduction to the Node.js reference architecture, Part 4: GraphQL in Node.js</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/tCiWOcx72HQ/introduction-nodejs-reference-architecture-part-4-graphql-nodejs" /><author><name>Wojciech Trocki</name></author><id>10f9bcc0-0deb-43fc-b6ef-989de0c9520a</id><updated>2021-06-22T07:00:00Z</updated><published>2021-06-22T07:00:00Z</published><summary type="html">&lt;p&gt;In this part of our ongoing introduction to the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture" target="_blank"&gt;Node.js reference architecture&lt;/a&gt;, we dig into some of the discussions the team had when developing the GraphQL section of the reference architecture. Learn about the principles we considered and gain additional insight into how we developed the current recommendations for using GraphQL in your Node.js applications.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Read the series so far&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p class="Indent1"&gt;&lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview" target="_blank"&gt;Part 1&lt;/a&gt;: Overview of the Node.js reference architecture&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p class="Indent1"&gt;&lt;a href="https://developer.ibm.com/languages/node-js/blogs/nodejs-reference-architectire-pino-for-logging/" target="_blank"&gt;Part 2&lt;/a&gt;: Logging in Node.js&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p class="Indent1"&gt;&lt;a href="https://developers.redhat.com/articles/2021/05/17/introduction-nodejs-reference-architecture-part-3-code-consistency" target="_blank"&gt;Part 3&lt;/a&gt;: Code consistency in Node.js&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Part 4&lt;/strong&gt;: GraphQL inÂ Node.js&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;GraphQL in the Node.js ecosystem&lt;/h2&gt; &lt;p&gt;GraphQL is a &lt;a href="https://graphql.org/learn/" target="_blank"&gt;query language specification&lt;/a&gt; that includes specific semantics for interaction between the client and server. Implementing a GraphQL server and client typically requires more effort than building REST applications, due to the extensive nature of the language and additional requirements for client-side and server-side developers. To start, let's consider a few of the elements of developing a Node.js application with GraphQL (Figure 1).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%282%29_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%282%29_0.png?itok=EeUSKWlI" width="600" height="643" alt=""We need to build a new app. Let's use GraphQL!" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Let's use GraphQL for our new app. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Developing a GraphQL schema&lt;/h3&gt; &lt;p&gt;When building a GraphQL API, client- and server-side teams must define strong contracts in the form of a GraphQL schema. The two teams must also change the way they have been communicating and developing their software. GraphQL internally requires server-side developers to build data-handling methods, called &lt;em&gt;resolvers&lt;/em&gt;, that match the &lt;em&gt;GraphQL schema&lt;/em&gt;, which is an internal graph that both teams must build and agree on. Client-side developers typically need to use specialized clients to send GraphQL queries to the back-end server.&lt;/p&gt; &lt;h3&gt;Choosing your tools&lt;/h3&gt; &lt;p&gt;The GraphQL ecosystem consists of thousands of libraries and solutions that you can find on GitHub, at conferences, and in various forums that offer to resolve all your GraphQL problems. On top of frameworks and libraries (Figure 2) the GraphQL ecosystem offers many out-of-the-box, self-hosted, or even service-based (SaaS) CRUD engines. Create, read, update, and delete (CRUD) engines offer to minimize the amount of server-side development by providing a direct link to the database. We'll come back to this topic later.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%283%29.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%283%29.png?itok=GHYpODOu" width="600" height="660" alt=""Should we use a service, framework, or CRUD engine?"" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: What tools will we use to enable GraphQL? &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Implementing a GraphQL API&lt;/h3&gt; &lt;p&gt;When implementing a GraphQL API, we often see a number of side-effects on other elements of our back-end infrastructure. A GraphQL API is typically exposed as a single endpoint by our back end, as illustrated in Figure 3.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-node-ref-4-fig-3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-node-ref-4-fig-3.png?itok=S1EFBsvN" width="600" height="397" alt="The visual shows that a GraphQL endpoint is exposed as a single endpoint." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Unlike a REST API, a GraphQL API is exposed as a single endpoint. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Adopting the GraphQL API means that we will not only need to change the API but will often have to rethink our entire infrastructure (Figure 4), from API management and security to caching, developing a federation of queries on gateways, and much more.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%287%29_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%287%29_1.png?itok=xcMaHNpT" width="600" height="663" alt="How do we get the product to work with the legacy back end?" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Think through your GraphQL-based application before implementing it. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Schema first or code first?&lt;/h3&gt; &lt;p&gt;There are multiple ways to develop GraphQL solutions. The two most common approaches are &lt;em&gt;schema first&lt;/em&gt;, where developers write GraphQL schema first and later build client-side queries and data resolvers on the back end, and &lt;em&gt;code first&lt;/em&gt; (also known as resolvers first), where developers write the resolvers first and then generate the GraphQL schema for them.&lt;/p&gt; &lt;p&gt;Both approaches come with advantages and disadvantages based on your specific use case.&lt;/p&gt; &lt;h2&gt;Implementing GraphQL for Node.js&lt;/h2&gt; &lt;p&gt;Making all of the decisions about how to implement GraphQL can be daunting, as illustrated by Figure 5.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%284%29.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%284%29.png?itok=mKvv77y7" width="600" height="672" alt=""Doing this on my own is really going to take time."" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Implementing GraphQL for Node.js is no simple task. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Many developers become overwhelmed by the amount of work required and look for libraries or tools that offer comprehensive support instead. As we've previously mentioned, in a GraphQL ecosystem, developers often look to one of the available CRUD engines for support (Figure 6).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%285%29.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%285%29.png?itok=enT_X-xt" width="600" height="660" alt=""Let's use a CRUD engine!"" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Using a CRUD engine is a tempting workaround. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Â &lt;/p&gt; &lt;p&gt;CRUD engines try to address the major shortcomings and complexity of GraphQL by offering unified and low-code data access. However, in the long run, they can fail to deliver the capabilities we want, especially integration with other services.&lt;/p&gt; &lt;p&gt;Moreover, the initial results associated with using productivity tooling are often the tip of the iceberg for what you will face when deploying your code to production (see Figure 7).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-node-ref-4-fig-7.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-node-ref-4-fig-7.png?itok=L_ANtheg" width="600" height="396" alt="Tools only address the surface issues of development." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: Considerations for developing a Node.js application with GraphQL. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Red Hat team members have been using GraphQL for many years, working with the community and customers to address different challenges encountered when using GraphQL, including those we've discussed in the preceding sections. Next, we'll introduce the GraphQL section of the Node.js Reference architecture, which is based on our experience as well as that of teams within IBM.&lt;/p&gt; &lt;h2&gt;GraphQL recommendations and guidance&lt;/h2&gt; &lt;p&gt;When working on the GraphQL section of the reference architecture, we discussed a number of principles and values that influenced the documented recommendations and guidance. Here, we'll offer a brief overview.&lt;/p&gt; &lt;h3&gt;Schema first development&lt;/h3&gt; &lt;p&gt;In order to support collaboration across different languages, microservices, and tools we recommend using the GraphQL schema as a form of API definition rather than generating a schema from the code. Code-first solutions typically are limited to a single language and can create compatibility issues between the front end and other useful GraphQL tools.&lt;/p&gt; &lt;h3&gt;Separate concerns&lt;/h3&gt; &lt;p&gt;When our back- and front-end codebase is minimal we can use tools to generate code, analyze our schemas, and so on. Those tools typically do not run in production but provide a number of features missing in the reference architecture. All elements should work outside your application and can be replaced if needed.&lt;/p&gt; &lt;h3&gt;Use the GraphQL reference implementation&lt;/h3&gt; &lt;p&gt;Using the GraphQL reference implementation facilitates supportability and it is vendor agnostic. GraphQL is a Linux Foundation project with a number of reference libraries maintained under its umbrella. Choosing these libraries over single vendor and product-focused open source libraries reduces the risk of providing support and maximizes the stability of our solutions over extended periods of time.&lt;/p&gt; &lt;h3&gt;Minimalism&lt;/h3&gt; &lt;p&gt;Developers often look for libraries that offer an improved API and increase productivity. In our experience, picking a high-level tool that focuses only on the essential elements needed to build a successful GraphQL API leads to the best outcome. As a result, we've chosen to include a very short list of packages and recommendations that are useful for developers.&lt;/p&gt; &lt;h3&gt;Exclude opinionated solutions&lt;/h3&gt; &lt;p&gt;The GraphQL section of the Node.js reference architecture does not include CRUD engines or tools that affect developer flexibility and introduce proprietary APIs.&lt;/p&gt; &lt;p&gt;Based on our discussion of these principles and values, along with our prior experience, we developed the recommendations and guidance captured in the reference architecture. We hope this article has given you some insight into the background and considerations the team covered in building that section. For more information, check out the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/functional-components/graphql.md" target="_blank"&gt;GraphQL section of the Node.js reference architecture&lt;/a&gt;.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/GraphQL%20story%20-%20Storyboard%20Example%20%286%29.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/GraphQL%20story%20-%20Storyboard%20Example%20%286%29.png?itok=gJm9je_w" width="600" height="662" alt="GraphQL works!" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: GraphQL works! &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Whatâs next?&lt;/h2&gt; &lt;p&gt;We plan to cover new topics regularly as part of the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/" target="_blank"&gt;Node.js reference architecture series&lt;/a&gt;. While you wait for the next installment, we invite you to visit the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture" target="_blank"&gt;Node.js reference architecture repository&lt;/a&gt; on GitHub, where you'll see the work weâve already done and the kinds of topics you can look forward to in the future.&lt;/p&gt; &lt;p&gt;To learn more about what Red Hat is up to on the Node.js front, check out our &lt;a href="https://www.redhat.com/en/topics/api/what-is-graphql" target="_blank"&gt;GraphQL&lt;/a&gt; or &lt;a href="https://developers.redhat.com/topics/nodejs" target="_blank"&gt;Node.js landing page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs" title="Introduction to the Node.js reference architecture, Part 4: GraphQL in Node.js"&gt;Introduction to the Node.js reference architecture, Part 4: GraphQL in Node.js&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/tCiWOcx72HQ" height="1" width="1" alt=""/&gt;</summary><dc:creator>Wojciech Trocki</dc:creator><dc:date>2021-06-22T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs</feedburner:origLink></entry><entry><title type="html">Calling All Roadies for the Quarkus World Tour</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/llNkBa0D-m8/" /><author><name /></author><id>https://quarkus.io/blog/calling-all-roadies/</id><updated>2021-06-22T00:00:00Z</updated><content type="html">The Quarkus World Tour kicked off in March to provide a unique, hands-on Quarkus experience for Java developers across the globe. The goal of the tour is to introduce Quarkus to Java developers and set them down the path of creating applications and participating in the community. Since the tour...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/llNkBa0D-m8" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/calling-all-roadies/</feedburner:origLink></entry><entry><title type="html">Kamelet for streaming to Kafka!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ckePpzU5Yyg/kamelet-for-streaming-to-kafka.html" /><author><name>CHRISTINA ã® Jèé</name></author><id>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/mmaYAtRaYw0/kamelet-for-streaming-to-kafka.html</id><updated>2021-06-21T14:10:00Z</updated><content type="html">You want Kafka to stream and process the data. But what comes after you set up the platform, planned the partitioning strategy, storage options, and configured the data durability? Yes! How to stream data in and out of the platform. And this is exactly what I want to discuss today.Â  THE BACKGROUND Before we go any further, letâs see what Kafka did to make itself blazing fast? Kafka is optimized for writing the stream data in binary format, that basically logs everything directly to the file system (Sequential I/O) and makes minimum effort to process what's in the data (Optimize for Zero Copy). Â Kafka is super-charged at making sure data is stored as quickly as possible, and quickly replicating for a large number of consumers. But terrible at communication, the client that pushes content needs to SPEAK Kafka.Â  Here we are, having a super fast logging and distributing platform, but dumb at connecting to other data sources. So who is going to validate the data sent in/out of the kafka topic? What if I need to transform the data content? Can I filter the content partially? You guessed it. The clients. Â We now need smart clients that do most of the content processing and speak Kafka at the same time.Â  WHAT ARE THE MOST USED CONNECT TOOLS TODAY FOR KAFKA USERS?Â  Kafka Connect is what the majority of the Kafka users are using today. It has been broken down into many parts such as connector, tasks, worker, converter, transformer and error handler. You can view the task and worker as how the data flow is executed. For a developer they will be mostly configuring the rest 4 pieces.Â  * Connector - Describes the kind of source or the sink of the data flow, translating between the client/Kafka protocol, and knowing the libraries needed.Â  * Converter - Converts the binary to the data format accepted by the client or vice versa Â  (Currently there is limited support from Confluent, they Â only do data format) And does data format validation.Â  * Transformer - Reads into the data format, can help make simple changes to individual data chunks. Normally you would do filtering, masking or any minor changes. (Â This does not support simple calculations) * Error Handler - Define a place to store problematic data (Confluent : Dead letter queues are only applicable for sink connectors.) After configuring, it then uses Task and Worker to determine how to scale and execute that pipe data in/out of Kaka. For instance, running a cluster of works to scale and allow tasks to perform parallel processing of streams.Â  Camel is anotherÂ greatÂ option! Apache Camel is aÂ GREAT alternativeÂ for connecting Kafka too. Hereâs what Camel has to offer.Â  * Connector - Camel has more than 300+ connectors, you can use it to configure as source or the sink of the data flow, translating between the 100+client/Kafka protocol. * Converter - Â Validate and transform data formats with simple configuration. * Transformer - Not only does simple message modification, it can apply integration patterns that are good for streaming processing, such as split, filter, even customization of processes. Â  * Error Handler - Dead letter queue, catching exceptions.Â  There are also many ways to run Camel. You can have it running as a standalone single process that directly streams data in/out of Kafka .Â But Kamel works EXCEPTIONALLY well on Kubernetes.Â It run as a cluster of instances, that execute in parallel to maximize the performance. It can be deployed as native image through Quarkus to increase density and efficiency. The platform OpenShift (Kubernetes) allows users to control the scaling of the instance. Since itâs on K8s, another advantage is that operation can operate these as a unify platform, along with all other microservices.Â  WHY KAMELET? Â (THIS IS THE WAY!) One of the biggest hurdles for non Camel developers is, they need to learn another framework, maybe another language (Non-Java) to be able to get Camel running. What if we can smooth the learning curve and make it simple for newcomers? We see a great number of use cases where the masking and filtering are implemented company wide. Being able to build a repository and reuse these logics will make developers work more efficiently.Â  PLUG &amp;amp; PLAYÂ  You can look at Kamelets as templates, where you can define where to consume data from and send data to, does filtering, masking, simple calculation logic. Once the template is defined, it can be made available to the teams, that simply plugs it into the platform, configure for their needs (with either Kamelet Binding or another Camel route), and boom. The underlying Camel K will do the hard work for you, compile, build, package and deploy. You have a smart running data pipeline streams into Kafka.Â  ASSEMBLE &amp;amp; REUSE In a data pipeline, sometimes, you just need that bit of extra work on the data. Instead of defining a single template for each case, you can also break it down into smaller tasks. And assemble these small tasks to perform in the pipeline for each use case. . Â Â  STREAMS &amp;amp; SERVERLESS Kamelets allows you to stream data to/from either Kafka store or Knative event channel/broker. To be able to support Knative, Kamelet can help translate messages to CloudEvents, which is the CNCF standard event format for serverless. And also apply any pre/post-processing of the content in the pipeline.Â  SCALABLE &amp;amp; FLEXIBLE Kamelet lives on Kubernetes(can also run standalone), which gives you a comprehensive set of scaling tools, readiness, liveness check and scaling configuration. They are all part of the package. It scales by adding more instances. The UI on the OpenShift Developer Console can assist you to fill in whatâs needed. And also auto discover the available source/sink for you to choose where the data pipelines start or end.Â  UNIFY FOR DEV &amp;amp; OPSÂ  In many cases, DevOps engineers are often required to develop another set of automation tools for the deployment of connectors. Kamelet can run like other applications on kubernetes, the same tools can be used to build, deploy and monitor these pipelines. The streamline DEVOPS experience can help speed up the automation setup time. Â  MARKETPLACE List of catalogues that are already available Â (Not enough?). If you just want to stream data directly, simple pick the ones you need and start streaming. And we welcome your contributions too. What to know more about Kamelet? Take a look at this video, where it talks about why using Kamelet for streaming data to Kafka with a demo.Â  Â Introduction Â What is Kamelet?Â  Â Why do you need connectors to Kafka, and what is required in each connector?Â  Â Why Kamelet?Â  Â Marketplace of Kamelet!Â  Â Using Kamelet as a Kafka userÂ  Â Building a KameletÂ  Â Running Kamelet on KubernetesÂ  Â DemoÂ  Â Red Hat OpenShift Streams in actionÂ  Â Kamelets in action&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ckePpzU5Yyg" height="1" width="1" alt=""/&gt;</content><dc:creator>CHRISTINA ã® Jèé</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/mmaYAtRaYw0/kamelet-for-streaming-to-kafka.html</feedburner:origLink></entry><entry><title type="html">An Infinispan .Net Core client over the Hot Rod protocol</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/W4mNLc6TGBA/infinispan-dotnet-core-client" /><author><name>Vittorio Rigamonti</name></author><id>https://infinispan.org/blog/2021/06/21/infinispan-dotnet-core-client</id><updated>2021-06-21T12:00:00Z</updated><content type="html">Dear Infinispanners, The Infinispan team would like to share a new project weâre working on: . Our current .NET client is based on the C++ core implementation, which is a solution that has its pros and cons. It makes it easier to guarantee parity between the C++ and C# clients over the time, but has the drawback of tying clients to specific a architecture. In turn that complicates portability and distribution, making the release lifecycle for those clients more onerous and sluggish. The is a 100% C# Hot Rod client designed with the aim of being portable across systems via the .Net Core platform and easier to deploy and consume via the Nuget platform. If you are involved with the .NET Core ecosystem we hope you will find this project of interest. Entry points for the project are: * , current status and news; * , this will contain the same testsuite of the current project with the aim of making it easier to compare with the .NET core client as it matures. * , an example of project which uses the client package. * is where the package is published. Hope this will makes C# developers happy! Please let us know your thoughts, a good place for them is the page. The Infinispan Team&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/W4mNLc6TGBA" height="1" width="1" alt=""/&gt;</content><dc:creator>Vittorio Rigamonti</dc:creator><feedburner:origLink>https://infinispan.org/blog/2021/06/21/infinispan-dotnet-core-client</feedburner:origLink></entry><entry><title>Modern Fortune Teller: Using GitOps to automate application deployment on Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/CYg36HRkzXs/modern-fortune-teller-using-gitops-automate-application-deployment-red-hat" /><author><name>Ken Lee, Preska Sharma, Keyvan Pishevar</name></author><id>e9039812-d068-4304-8860-390fbe4ac2a2</id><updated>2021-06-21T07:00:00Z</updated><published>2021-06-21T07:00:00Z</published><summary type="html">&lt;p&gt;Our team recently created an application called Beer Horoscope, which we used to illustrate the extensive possibilities for modern software development and deployment with &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; and community-built free software tools. The application's front end collects user preferencesÂ and makes beer recommendations. The back end performsÂ &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;machine learning&lt;/a&gt;Â on users and products (beers) to make appropriate recommendations.Â Figure 1 shows howÂ we combined an &lt;a href="https://developers.redhat.com/topics/event-driven"&gt;event-driven architecture&lt;/a&gt; with machine learning models that are applicable to numerous real-world scenarios.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/beer_cycle.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/beer_cycle.png?itok=5PegWzRq" width="600" height="209" alt="In the Beer Horoscope application, we run a cycle of analyzing new ratings, retraining models, and redeploying the service." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The application flow of data collection, analysis, and service redeployment. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This article summarizes our talk at the 2021 Red Hat Summit break-out session titled &lt;a href="https://events.summit.redhat.com/widget/redhat/sum21/sessioncatalog/session/1607116397279001IHS4"&gt;Modern Fortune Teller: Your Beer Horoscope with AI/ML&lt;/a&gt;. We'll discuss how we used &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; on OpenShift, along with ArgoCD, to continuously deploy our application as we were developing it. We'll also explain how we used &lt;a href="https://opendatahub.io"&gt;Open Data Hub&lt;/a&gt; as a one-stop machine learning environment to create and test our algorithms on OpenShift. See our &lt;a href="https://github.com/beer-horoscope/beer-horoscope"&gt;GitHub repository&lt;/a&gt; for application source code and additional documentation.&lt;/p&gt; &lt;h2&gt;What is GitOps?&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2021/05/13/why-should-developers-care-about-gitops"&gt;GitOps&lt;/a&gt; is a way of implementing &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous deployment&lt;/a&gt; (CD) for cloud-native applications. GitOps extends CD from development to cloud deployment using tools developers are already familiar with, including Git.&lt;/p&gt; &lt;p&gt;The core idea of GitOps is to create a Git repository that contains declarative descriptions of the infrastructure. These are updated so they always indicate the images currently desired in the production environment, as shown in Figure 2.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/gitops-flow.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/gitops-flow.png?itok=seT3Gs50" width="392" height="414" alt="GitOps continuously evaluates the desired state of production systems and automatically updates those systems in the cloud." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Flow of events in GitOps that keeps production systems up to date in the cloud. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;GitOps's advantages in practice include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Faster and more frequent application deployments&lt;/li&gt; &lt;li&gt;Easier and faster error recovery&lt;/li&gt; &lt;li&gt;Simplified credential management&lt;/li&gt; &lt;li&gt;Self-documenting deployments&lt;/li&gt; &lt;li&gt;Shared knowledge throughout the team&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Red Hat's GitOps Operator&lt;/h2&gt; &lt;p&gt;Red Hat OpenShift is a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; platform meeting the declarative principles that &lt;a href="https://docs.openshift.com/container-platform/4.7/cicd/gitops/understanding-openshift-gitops.html"&gt;allow administrators to configure and manage deployments using GitOps&lt;/a&gt;. Working within a Kubernetes-based infrastructure and applications, you can apply consistency across clusters and development life cycles.&lt;/p&gt; &lt;p&gt;Red Hat collaborates with open source projects such as &lt;a href="https://argoproj.github.io/argo-cd/"&gt;Argo CD&lt;/a&gt; and &lt;a href="https://github.com/tektoncd/pipeline"&gt;Tekton Pipeline&lt;/a&gt; to implement a framework for GitOps.&lt;/p&gt; &lt;p&gt;For this application, we leveraged Red Hat's &lt;a href="https://github.com/redhat-developer/gitops-operator"&gt;GitOps Operator&lt;/a&gt; for application deployment. This operator allows for continuous updates and delivery via ArgoCD and Git, thus implementing GitOps.&lt;/p&gt; &lt;p&gt;ArgoCD pulls the deployment instructions from our Git repository and installs &lt;a href="https://developers.redhat.com/products/amq/getting-started"&gt;Red Hat AMQ Streams&lt;/a&gt;. AMQ Streams is based on the &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; streaming tool. The AMQ Streams Operator allows developers to use Kafka and various components, such as &lt;a href="https://docs.confluent.io/home/connect/overview.html"&gt;Kafka Connectors&lt;/a&gt;, to support complex event processing. For this project, we use AMQ Streams in high availability mode.&lt;/p&gt; &lt;h2&gt;The Open Data Hub Operator&lt;/h2&gt; &lt;p&gt;We used Open Data Hub as a one-stop environment for machine learning and artificial intelligence (&lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;AI/ML&lt;/a&gt;) services and tools on OpenShift. Open Data Hub provides tools at every stage of the AI/ML workflow, and for multiple user personas including data scientists, DevOps engineers, and software engineers. The &lt;a href="https://gitlab.com/opendatahub/opendatahub-operator"&gt;Open Data Hub Operator&lt;/a&gt; (Figure 3) lets developers use a best-of-breed machine learning toolset and focus on building the application.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/open_data_hub.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/open_data_hub.png?itok=OkszwMB-" width="600" height="339" alt="The Open Data Hub Operator gave the Beer Horoscope project access to a range of tools, including operators for AMQ Streams, Prometheus, and others." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Tools provided by the Open Data Hub Operator to the Beer Horoscope project. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Figure 4 shows the platform components, such as Jupyter Notebook, Apache Spark, and others, that Open Data Hub makes available for data scientists via Kubeflow.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kfdef.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/kfdef.png?itok=nqIBL8jy" width="600" height="395" alt="Kubeflow provides a wide range of open source tools for data processing and analysis." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Data processing and analysis tools provided by Kubeflow. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Developing the recommendation system&lt;/h2&gt; &lt;p&gt;To develop the algorithms for the Beer Horoscope project, we needed a recommendation system. To choose the correct algorithms for the system, we first had to determine the relationships involved when users get a beer recommendation. Three main types of relationships occur in this scenario:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;User-product: What kind of beer does this user like to drink?&lt;/li&gt; &lt;li&gt;Product-product: What beers are similar to each other?&lt;/li&gt; &lt;li&gt;User-user: Which users have similar taste in beer?&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Two of these relationships can be established by two very popular algorithms used in recommendation systems:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Collaborative filtering&lt;/strong&gt;: Used to establish user-user relationships. If one user rates Beer A very highly, and another user also rates Beer A very highly, we can assume these users have similar taste in beer. We can then start recommending to each user the beers that are highly rated by the other user.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Content-based filtering&lt;/strong&gt;: Used to establish product-product relationships. If a user likes Beer A, it's safe to recommend beers similar to Beer A to the user.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We developed models using these two algorithms on &lt;a href="https://jupyter.org/hub"&gt;JupyterHub&lt;/a&gt; through Open Data Hub. We had access there to all of the components that made up the development environment, including environment variables, databases, configurations settings, and so on.&lt;/p&gt; &lt;p&gt;Once we created our models, our application deployment life cycle became very similar to the software development life cycle.&lt;/p&gt; &lt;p&gt;Next, we'll take a look at how cloud-native development utilizes containers, &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;continuous delivery&lt;/a&gt;, and &lt;a href="topics/microservices"&gt;microservices&lt;/a&gt; to automate these formerly time-consuming steps.&lt;/p&gt; &lt;h2&gt;How we built the Beer Horoscope&lt;/h2&gt; &lt;p&gt;Up to this point, we've covered how we automated the infrastructure that our application runs on, from the perspective of a &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt; engineer, by leveraging GitOps. We then discussed how we trained and created the data models from the perspective of a data engineer and data scientist.&lt;/p&gt; &lt;p&gt;Here, we turn to the viewpoint of an application developer. We'll go over how an application interacts with trained data models and how to leverage the OpenShift infrastructure and platform to make these interfaces possible.&lt;/p&gt; &lt;p&gt;The AMQ Streams Operator, GitOps Operator, and Open Data Hub Operator, discussed earlier, were all available from the OperatorHub, shown in Figure 5.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/operatorhub_tools.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/operatorhub_tools.png?itok=6aUGErRS" width="600" height="613" alt="A number of open source, community-based tools came together in Open Data Hub and are exposed by OpenShift as Operators in OperatorHub." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Community-based tools that contribute to OperatorHub. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Application architecture&lt;/h2&gt; &lt;p&gt;Figure 6 lists the systems involved in this architecture and roughly indicates how they relate to each other. The application components are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Users&lt;/strong&gt;: These are evaluated for their preferences and are served by the project.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Applications and services tier&lt;/strong&gt;: This collection typically houses artifacts such as web applications, REST services, and internal services. Much of the APIs and underlying business logic were extrapolated from Jupyter notebooks developed by data engineers. The front-end component was written using &lt;a href="https://vuejs.org/"&gt;Vue.js&lt;/a&gt;. The API services are built on &lt;a href="search?t=python"&gt;Python&lt;/a&gt; and &lt;a href="https://flask.palletsprojects.com/"&gt;Flask&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data tier&lt;/strong&gt;: This tier stores both raw and structured data, as well as trained data models. This data is used by the consuming tiers. The Beer Horoscope application uses both the &lt;a href="https://www.mysql.com/"&gt;MySQL&lt;/a&gt; relational database and file storage to store data.&lt;/li&gt; &lt;li&gt;Event-processing tier: In this tier, we orchestrate how we process any new data introduced into our ecosystem. We create data streams to handle complex event processing scenarios and business rules. For this tier, we used Kafka Connectors and &lt;a href="https://kafka.apache.org/documentation/streams/"&gt;Streams&lt;/a&gt; for complex event processing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Logging, monitoring, and analytics&lt;/strong&gt;: This tier provides functions for logging and monitoring, so that we can analyze what's going on in real time and keep historical records. This tier used the &lt;a href="https://operatorhub.io/operator/grafana-operator"&gt;Grafana&lt;/a&gt; and &lt;a href="https://operatorhub.io/operator/prometheus"&gt;Prometheus&lt;/a&gt; Operators.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Container registry&lt;/strong&gt;: To facilitate the versioning, storage, and retrieval of container image artifacts in our OpenShift cluster, we stored all application image artifacts within a container registry. The Beer Horoscope uses &lt;a href="https://quay.io/"&gt;Quay.io&lt;/a&gt; to host these artifacts.&lt;/li&gt; &lt;/ul&gt;&lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/beer_architecture.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/beer_architecture.png?itok=6mm1TBRz" width="600" height="384" alt="The Beer Horoscope application includes many components and tiers, described in the text." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Components of the Beer Horoscope application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, we explored the tools and methods we used to operationalize our machine learning models, and how we then brought algorithms written on a Jupyter notebook into production through a user-friendly web application.&lt;/p&gt; &lt;p&gt;Before starting a full-stack project, you need an environment that supports continuous delivery. We used Red Hat's GitOps Operator on OpenShift, along with ArgoCD, to continuously deploy our application as we were developing it.&lt;/p&gt; &lt;p&gt;Then, we used OpenShift's Open Data Hub Operator as a one-stop machine learning environment to create and test our algorithms. The MySQL databases and file system store our large datasets and trained data models, respectively.&lt;/p&gt; &lt;p&gt;Finally, we created an application that interacts with our models. Our full-stack application includes the front-end user interface, API services written in Flask that talk to our machine-learning model training services written in Python, the data tier, and the event-processing tier that uses Kakfa Streams through AMQ Streams.&lt;/p&gt; &lt;p&gt;By closely examining and optimizing the software development cycle, we were able to collaborate and deploy into production an intelligent applicationâone that's telling you to go grab a cold beer right now!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/21/modern-fortune-teller-using-gitops-automate-application-deployment-red-hat" title="Modern Fortune Teller: Using GitOps to automate application deployment on Red Hat OpenShift"&gt;Modern Fortune Teller: Using GitOps to automate application deployment on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/CYg36HRkzXs" height="1" width="1" alt=""/&gt;</summary><dc:creator>Ken Lee, Preska Sharma, Keyvan Pishevar</dc:creator><dc:date>2021-06-21T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/21/modern-fortune-teller-using-gitops-automate-application-deployment-red-hat</feedburner:origLink></entry><entry><title type="html">Cloud adoption - Common architectural elements</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/2JAH4BfNY0A/cloud-adoption-common-architectural-elemetns.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/MLExTSq6JLQ/cloud-adoption-common-architectural-elemetns.html</id><updated>2021-06-21T05:00:00Z</updated><content type="html">Part 2 - Common architectural elements InÂ ourÂ Â from this series we introduced a use case around cloud adoption for retail stores. The process was laid out how we've approached the use case and how portfolio solutions are the base for researching a generic architecture.Â  The only thing left to cover was the order in which you'll be led through the architectural details. This article starts the real journey at the very top, with a generic architecture from which we'll discuss the common architectural elements one by one. This will start our journey into the logical elements that make up the cloud adoption architecture. ARCHITECTURE REVIEW As mentioned before, the architectural details covered here are base on real solutions using open source technologies. The example scenario presented here is aÂ generic commonÂ architecture that was uncovered researching those solutions. It's our intent to provide guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architecture, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, orÂ Â with your feedback. FROM SPECIFIC TO GENERIC Before diving in to the common elements, it might be nice to understand that this is not a catch all for every possible solution. It's a collection of identified elements that we've uncovered in multiple customer implementations. These elements presented here are then theÂ generic common architectural elementsÂ that we've identified and collected in to the generic architecture.Â  It's our intent to provide an example for guidance and not deep technical details. You're smart enough to figure out wiring integration points in your own architectures. You're capable of slotting in the technologies and components you've committed to in the past where applicable.Â  It's our job here to describe the architecture generic components and outline a few specific cases with visual diagrams so that you're able to make the right decisions from the start of your own projects. Another challenge has been how to visually represent the architecture. There are many ways to represent each element, but we've chosen some icons, text and colours that we hope are going to make it all easy to absorb. Now let's take a quick tour of the generic architecture and outline the common elements uncovered in my research. CORE DATA CENTER The logical view splits this solution space into several identifiable collections where the cloud adoption solution is laid out. These logical collections ensure that your organisation can provide effective automation for deploying and managing workloads across multiple cloud infrastructures according to performance, security, compliance, and cost requirements. The first collection on the left is tagged as the core data centerÂ and holds all the logical elements needed to put together the images for your infrastructure and workloads to run on. You find a source code management (SCM) system,Â an image store, and the server image build pipeline. All elements used by an organisation to create, manage, store, and testing images for distribution. INFRASTRUCTURE MANAGEMENT TheÂ infrastructure managementÂ collection is where intelligence is gathered, monitoring is performed, and based on the findings, triggers automated reactions and orchestrates updates to your infrastructure anywhere in your organisation. AÂ smart managementÂ element is used for tracking, managing, auditing, and collecting data on your entire infrastructure to ensure that baselines are met. Based on your choices and the results of data collected, your insights trigger corrections, updates, or even rolling out of new infrastructure across any of the cloud infrastructure your organisation might be using. The automation orchestration element is tasked with orchestration of infrastructure tasks in a fully automated and pre-tested fashion. This element is directed to execute certain tasks in a certain order based on the findings of the smart managementÂ element. CLOUD INFRASTRUCTURE This collection of elements are all aspects of an organisations cloud infrastructure. The idea is that organisations are at the very least moving to put the cloud-native experience together for their development teams to execute on their business goals while supporting an agile customer experience.Â  To provide a cloud experience, existing physical data center resources might be the starting point to be offered to the organisation with a cloud-like experience. Once this is completed, the organisation then has a private cloud to build, test, and run its workloads on.Â  Finally, as needed, organisations can expand services and applications out into one or more of the public cloudÂ providers. All of these are shown here as aÂ Red Hat Enterprise Linux (RHEL) hostÂ element with an image registryÂ to facilitate the deployment of infrastructure, services, and applications across the entire hybrid cloud infrastructure. CLOUD SERVICES Last but not least, there is a need for cloud servicesÂ that can facilitate all it takes to span the monitoring, analysing, and deployment of an organisations workloads across their hybrid cloud infrastructure. The first element is that of enterprise operating automationÂ which facilitates consistent, repeatable, and tested infrastructure automation tasks as needed by the other elements managing the hybrid cloud infrastructure.Â  Next, there is the insights platform. This is key to monitoring and data collection around the entire hybrid cloud infrastructure. Based on this data and working together with insights services, automated actions can take place around updates, security patches, infrastructure rollouts, workload management, and workload migrations. This is the key to an organisations ability to successfully adopt a truly hybrid cloud infrastructure. WHAT'S NEXT This was just a short overview of the common generic elements that make up our architecture for the cloud adoption use case.Â  An overview of this series on the cloud adoption portfolio architecture: 1. 2. 3. Example adoption architecture Catch up on any past articles you missed by following any published links above. Next in this series, taking a look at an example adoption architecture. (Article co-authored byÂ , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/2JAH4BfNY0A" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/MLExTSq6JLQ/cloud-adoption-common-architectural-elemetns.html</feedburner:origLink></entry><entry><title>Perform a kaniko build on a Red Hat OpenShift cluster and push the image to a registry</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/w5sV_LpWTN0/perform-kaniko-build-red-hat-openshift-cluster-and-push-image-registry" /><author><name>Jaideep Rao</name></author><id>764338ac-d9a4-4733-a077-4936b2d6274d</id><updated>2021-06-18T07:00:00Z</updated><published>2021-06-18T07:00:00Z</published><summary type="html">&lt;p&gt;Typically, building &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; and images from a standard &lt;a href="https://docs.docker.com/engine/reference/builder/"&gt;Dockerfile&lt;/a&gt; requires root access and permissions. This can create a challenge when working with public or shared clusters. For example, cluster admins don't often allow permissions to run this type of workload, as it might compromise the security of the entire cluster.&lt;/p&gt; &lt;p&gt;In these situations, many developers use a build tool such as &lt;a href="https://github.com/GoogleContainerTools/kaniko"&gt;kaniko&lt;/a&gt; to simplify the effort. Kaniko can build your images without requiring root access. This capability makes kaniko a feasible alternative for building containers and images in any kind of environment; for example, standard &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; clusters, Google Kubernetes Engine, and public or shared clusters. Kaniko can also automatically push your images to a specified image registry.&lt;/p&gt; &lt;p&gt;This article shows you how to use kaniko to build a container image in a &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; cluster and push the image to a registry.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To perform a kaniko build on a Red Hat OpenShift cluster, ensure that the following prerequisites are in place:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Access to an active OpenShift cluster (with admin access).&lt;/li&gt; &lt;li&gt;Access to a source code repository that is either local or hosted somewhere, such as GitHub.&lt;/li&gt; &lt;li&gt;A valid Dockerfile for your target source directory. The Dockerfile can exist anywhere, as long as a fully-qualified URL is available.&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: All the &lt;code&gt;oc&lt;/code&gt; commands in this article also work with &lt;code&gt;kubectl&lt;/code&gt;, whether you are working with an OpenShift cluster or a Kubernetes cluster, or without a cluster.&lt;/p&gt; &lt;h2&gt;Setup and configuration for kaniko on OpenShift&lt;/h2&gt; &lt;p&gt;Once the prerequisites are set up, configured, and active, you can perform a kaniko build on an OpenShift cluster and push the image to a registry.&lt;/p&gt; &lt;h3&gt;Log in to the OpenShift cluster&lt;/h3&gt; &lt;p&gt;To start, log in to your OpenShift cluster as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc login --token=token --server=server-url&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Create a new project&lt;/h3&gt; &lt;p&gt;Create your own project using:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project project-name&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Create a secret using the credentials to your registry&lt;/h3&gt; &lt;p&gt;To push your image to an external registry (such as Docker Hub), create a secret named &lt;code&gt;regcred&lt;/code&gt; using the following &lt;code&gt;oc&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret docker-registry regcred \ --docker-server=your-registry-server \ --docker-username=your-name \ --docker-password=your-pword \ --docker-email=your-email&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Replace the italicized values in this command with the following information:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;&lt;em&gt;your-registry-server&lt;/em&gt;&lt;/code&gt;: the fully qualified domain name (FQDN) of your private Docker registry (https://index.docker.io/v1/ for Docker Hub)&lt;/li&gt; &lt;li&gt;&lt;em&gt;&lt;code&gt;your-name&lt;/code&gt;&lt;/em&gt;: your Docker username&lt;/li&gt; &lt;li&gt;&lt;em&gt;&lt;code&gt;your-pword&lt;/code&gt;&lt;/em&gt;: your Docker password&lt;/li&gt; &lt;li&gt;&lt;em&gt;&lt;code&gt;your-email&lt;/code&gt;&lt;/em&gt;: your Docker email address&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Push your image to an internal registry through a pod on a cluster using a service account. For example, you can acquire login credentials for your service account, such as for Builder, through the cluster's console.&lt;/p&gt; &lt;p&gt;From the list of available secrets in your namespace, pick a &lt;code&gt;builder-dockercfg&lt;/code&gt; secret, and expose the base64 credentials using the &lt;strong&gt;Reveal Values&lt;/strong&gt; button on the OpenShift console.&lt;/p&gt; &lt;p&gt;Locate the URL for your target image registry and copy the authorization token. Use it to prepare a new &lt;code&gt;config.json&lt;/code&gt; file by replacing&lt;em&gt; image-registry-url &lt;/em&gt;and &lt;em&gt;auth-token&lt;/em&gt; with the appropriate values. For example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;{ "auths": { "image-registry-url": { "auth": "auth-token" } }, "HttpHeaders": { "User-Agent": "Docker-Client/19.03.8 (darwin)" }, "experimental": "disabled" }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the &lt;code&gt;config.json&lt;/code&gt; file is ready, create a secret as follows, naming it &lt;code&gt;regcred&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic regcred --from-file=.dockerconfigjson=path/to/config.json --type=kubernetes.io/dockerconfigjson&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Clone a source code repository&lt;/h3&gt; &lt;p&gt;In the local file system, &lt;code&gt;git clone&lt;/code&gt; your source code repository. For example, in an empty directory enter the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git://github.com/openshift/golang-ex.git&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, download the corresponding Dockerfile from its location and place it at the root of this directory, if it doesn't already exist there.&lt;/p&gt; &lt;p&gt;If a specific subdirectory within your cloned repo hosts the code used to build an image (as opposed to the entire cloned directory), place the Dockerfile at the root of that subdirectory. Together the directory containing your source code and Dockerfile now represent your build context.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: When using the Dockerfile present in the mentioned repository, drop the line that says &lt;code&gt;USER nobody&lt;/code&gt;, to avoid permission issues.&lt;/p&gt; &lt;p&gt;Make sure that the paths mentioned after &lt;code&gt;/kaniko/build-context&lt;/code&gt; against the &lt;code&gt;--dockerfile&lt;/code&gt; and &lt;code&gt;--context&lt;/code&gt; parameters in the &lt;code&gt;openshift-pod.yaml&lt;/code&gt; file accurately represents the directory structure present inside &lt;code&gt;kaniko-build-context.tar.gz&lt;/code&gt;. The paths must be an exact match.&lt;/p&gt; &lt;h3&gt;Compress the build context into a tar.gz file&lt;/h3&gt; &lt;p&gt;Once the build context is ready, compress it into a &lt;code&gt;tar.gz&lt;/code&gt; file as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ tar -czvf kaniko-build-context.tar.gz path/to/folder&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Create an openshift-pod.yaml file with two containers&lt;/h3&gt; &lt;p&gt;Create an &lt;code&gt;openshift-pod.yaml&lt;/code&gt; file that has two containers, as shown in Figure 1.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/blog/2020/12/kaniko-blog.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/blog/2020/12/kaniko-blog.png?itok=7ezxtVYs" width="600" height="628" title="kaniko-blog" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: An openshift-pod.yaml file with two containers.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Â &lt;/p&gt; &lt;p&gt;If you are pushing to Docker Hub, you could set the destination as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;--destination=docker.io/your-dockerhub-username/image-name:image-tag&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you are pushing to the internal registry, set the destination as shown here:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;--destination=image-registry.openshift-image-registry.svc:5000/your-project-name/image-name:image-tag&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Apply the pod to the cluster&lt;/h3&gt; &lt;p&gt;Use the following command to apply the pod to your cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f path/to/openshift-pod.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The command should return:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;pod/kaniko created&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Check the status of the cluster&lt;/h3&gt; &lt;p&gt;To check the cluster's status, run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get pods&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is an example of what it displays:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;NAME READY STATUS RESTARTS AGE kaniko 0/1 Init:0/1 0 50s&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Copy tar.gz from the local file system to the kaniko-init container&lt;/h3&gt; &lt;p&gt;Copy the &lt;code&gt;tar.gz&lt;/code&gt; file that you created earlier from the local file system to the &lt;code&gt;kaniko-init&lt;/code&gt; container currently running in the pod:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc cp path/to/kaniko-build-context.tar.gz kaniko:/tmp/kaniko-build-context.tar.gz -c kaniko-init&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Extract the copied tar.gz file on the mounted path to the shared volume&lt;/h3&gt; &lt;p&gt;From inside your &lt;code&gt;kaniko-init&lt;/code&gt; container, extract the copied &lt;code&gt;tar.gz&lt;/code&gt; file into the mounted path pointing to the shared volume inside of the kaniko pod. This allows it to be accessed by other containers with access to this shared volume.&lt;/p&gt; &lt;h2&gt;Check your work&lt;/h2&gt; &lt;p&gt;You should see the pushed image reflected in your target registry. Additionally, you can take a closer look inside the container at any time. (I found this to be quite useful while attempting to debug the process.) To begin, start a bash session inside your &lt;code&gt;kaniko-init&lt;/code&gt; container and take a look:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc exec kaniko -c kaniko-init -it /bin/bash&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the extraction process is complete, you can shut down the init container, at which point the kaniko container takes over. Then create a file that serves as a trigger:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc exec kaniko -c kaniko-init -- touch /tmp/complete&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When you run &lt;code&gt;oc get pods&lt;/code&gt; again, the output displays whether everything is working well:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;NAME READY STATUS RESTARTS AGE kaniko 1/1 Running 0 6m57s&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, run the following &lt;code&gt;oc&lt;/code&gt; command to get a more detailed look at what's going on inside the kaniko container:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc describe pod kaniko&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you can look at the pod logs within the OpenShift console.&lt;/p&gt; &lt;p&gt;After the pod has reached a completed state, if you pushed it to an external registry, you should be able to log into your registry and find the newly pushed image there. If you pushed to the internal registry, you should be able to navigate to &lt;strong&gt;Builds â&gt;&lt;/strong&gt; &lt;strong&gt;ImageStreams &lt;/strong&gt;(within the OpenShift console's Administrator view) to find the newly pushed image there.&lt;/p&gt; &lt;p&gt;You can delete the pod if needed using &lt;code&gt;oc delete pod kaniko&lt;/code&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/18/perform-kaniko-build-red-hat-openshift-cluster-and-push-image-registry" title="Perform a kaniko build on a Red Hat OpenShift cluster and push the image to a registry"&gt;Perform a kaniko build on a Red Hat OpenShift cluster and push the image to a registry&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/w5sV_LpWTN0" height="1" width="1" alt=""/&gt;</summary><dc:creator>Jaideep Rao</dc:creator><dc:date>2021-06-18T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/18/perform-kaniko-build-red-hat-openshift-cluster-and-push-image-registry</feedburner:origLink></entry><entry><title type="html">This Week in JBoss - 18 June 2021</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/yUc39IMGP5o/weekly-2021-06-18.html" /><category term="quarkus" /><category term="wildfly" /><category term="keycloak" /><category term="kogito" /><category term="infinispan" /><category term="vert.x" /><category term="java" /><category term="narayana" /><author><name>Don Naro</name><uri>https://www.jboss.org/people/don-naro</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2021-06-18.html</id><updated>2021-06-18T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, wildfly, keycloak, kogito, infinispan, vert.x, java, narayana"&gt; &lt;h1&gt;This Week in JBoss - 18 June 2021&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hello! Welcome to another edition of the JBoss Editorial that brings you news and updates from our community.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Congrats to all the teams on their hard work!&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2021/06/keycloak-1400-released"&gt;Keycloak 14.0.0&lt;/a&gt; is released! This release adds Financial-grade API (FAPI) support and lots of improvements.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://bytemanblog.blogspot.com/2021/06/byteman-4016-has-been-released.html"&gt;Byteman 4.0.16&lt;/a&gt; has shipped and is now the latest version if youâre running Java version 9 to 17.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org//news/2021/06/17/WildFly24-Final-Released/"&gt;WildFly 24&lt;/a&gt; is here and brings lots of awesome work. Thereâs too much to sum up here so click the link and check out the release notes.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-0-0-cr3-released/"&gt;Quarkus 2.0.0.CR3&lt;/a&gt; is here. Towards 2.0 final!!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-1-13-7-final-released/"&gt;Quarkus 1.13.7.Final&lt;/a&gt; is available as a maintenance release.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-4-1-0/"&gt;Eclipse Vert.x 4.1.0&lt;/a&gt; is available with lots of exciting features. Go download and start using it!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-3-9-8/"&gt;Eclipse Vert.x 3.9.8&lt;/a&gt; is also here with numerous bug fixes.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://jbossts.blogspot.com/2021/06/narayana-5120final-released.html"&gt;Narayana 5.12.0.Final&lt;/a&gt; has been shipped. This version gives several enhancements and fixes some bugs. Go grab it!&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_from_the_community"&gt;From the community&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Katia and Ryan on the Infinispan team released a nice two-part series of posts that explained how they built an online game across data centers and multiple cloud providers for this yearâs Summit keynote demonstration. In the first part, &lt;a href="https://developers.redhat.com/articles/2021/05/28/building-real-time-leaderboard-red-hat-data-grid-and-quarkus-hybrid-kubernetes"&gt;Building a real-time leaderboard with Infinispan and Quarkus on a hybrid Kubernetes deployment&lt;/a&gt;, Katia and Ryan describe how they designed and implemented various services using Infinispan and Quarkus. With part two, they explain how using the Infinispan Operator greatly reduced the complexity of standing up their clusters, &lt;a href="https://developers.redhat.com/articles/2021/06/08/create-and-manage-red-hat-data-grid-services-hybrid-cloud"&gt;Creating and managing Infinispan services in the hybrid cloud&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Tommaso Teofiliâs recent post, &lt;a href="https://blog.kie.org/2021/06/autotuning-lime-explanations-with-few-predictions.html"&gt;Autotuning LIME explanations with few predictions&lt;/a&gt;, outlines how to automatically tune LIME hyperparameters to achieve more stable explanations and comes with a PR that lets you dig into all the technical aspects covered in the post.&lt;/p&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2021/06/rhpam-connectivity-to-external-amq-configurations-in-openshift.html"&gt;RHPAM connectivity to external AMQ configurations in OpenShift&lt;/a&gt; by Michael Perez takes an in-depth look at connecting to an external AMQ with the aim of lowering the memory footprint for RHPAM pods as well as other deployment optimizations. Itâs an interesting read with good technical considerations to sink your teeth into so go give it a look.&lt;/p&gt; &lt;p&gt;Over on the WildFly blog, Jeff Mesnil walks us through the process of changing logging levels for cloud-based WildFly applications on the fly. Take a look at Jeffâs script and straightforward commands to help you easily modify logs for debugging in his post, &lt;a href="https://www.wildfly.org/news/2021/06/15/change-log-level-wildfly-cloud/"&gt;How to Change Logging Level for WildFly on the Cloud&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Bilgin Ibryam is back with a post on the Red Hat Developer blog titled &lt;a href="https://developers.redhat.com/articles/2021/06/14/application-modernization-patterns-apache-kafka-debezium-and-kubernetes"&gt;Application modernization patterns with Apache Kafka, Debezium, and Kubernetes&lt;/a&gt;. I really enjoyed this one as Bilgin expertly puts application modernization in context and examines different patterns, tools, and open-source ecosystems that can help you migrate brown-field systems to more modern, event-driven services as well as design green-field services that are future proof by providing the ability to evolve over time. Be sure to catch up on this one if you havenât already read it.&lt;/p&gt; &lt;p&gt;Just in time for your summer reading list the first book dedicated to Keycloak has been published, &lt;a href="https://www.keycloak.org/2021/06/book.adoc"&gt;Keycloak - Identity and Access Management for Modern Applications&lt;/a&gt;. Congrats to authors Stian Thorgersen and Pedro Igor Silva. Itâs an impressive achievement and no doubt the book is full of invaluable expertise.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_evangelists_corner"&gt;Evangelistâs corner&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;In our last editorial we mentioned Eric Schabellâs series on a retail data framework. Since then Eric has obviously been hard at work as his next two posts in the series, &lt;a href="https://www.schabell.org/2021/05/retail-data-framework-common-architectural-elements.html"&gt;Retail data framework - Common architectural elements&lt;/a&gt; and &lt;a href="https://www.schabell.org/2021/06/retail-data-framework-example-data-architecture.html"&gt;Retail data framework - Example data architecture&lt;/a&gt;. If you havenât caught up on that series yet, then I highly recommend taking a look to learn about data flows and management in a retail context.&lt;/p&gt; &lt;p&gt;Eric has also posted the first in his next series, &lt;a href="https://www.schabell.org/2021/05/cloud-adoption-an-architectural-introduction.html"&gt;Cloud adoption - An architectural introduction&lt;/a&gt;. This series is focused on proven integrations, structures, and interactions with the goal of enabling readers to implement and adopt cloud-based solutions using open-source technologies. Iâm really excited about this series and canât wait to see what Eric brings in his next posts.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_developers_on_film"&gt;Developers on film&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Get your popcorn ready and sit back to watch some videos from our community. Here are my top picks for this weekâs editorial:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/ETTMBWEBfLY"&gt;Quarkus Insights #51: Answering questions from the community&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/pYhaZYX0kq4"&gt;Quarkus Insights #53: Java Memory - why should you care&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/ngXii5sA_nA"&gt;Building Kubernetes Native Java with the Quarkus CLI&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/don-naro.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Don Naro&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/yUc39IMGP5o" height="1" width="1" alt=""/&gt;</content><dc:creator>Don Naro</dc:creator><feedburner:origLink>https://www.jboss.org/posts/weekly-2021-06-18.html</feedburner:origLink></entry><entry><title>How to deliver decision services with Kogito</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/iMnvBPVvqYA/how-deliver-decision-services-kogito" /><author><name>Karina Varela</name></author><id>66d60789-4d39-441c-9382-40faea7db139</id><updated>2021-06-17T07:00:00Z</updated><published>2021-06-17T07:00:00Z</published><summary type="html">&lt;p&gt;This article is the first of two presenting new support for developing decision services in &lt;a href="https://developers.redhat.com/products/rhpam/overview"&gt;Red Hat Business Automation Manager&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/rhpam/overview"&gt;Red Hat Process Automation Manager&lt;/a&gt;. We specifically address support for the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_decision_manager/7.1/html/designing_a_decision_service_using_dmn_models/dmn-support-con_dmn-models"&gt;Decision Model and Notation&lt;/a&gt; (DMN) standard. Process Automation Manager now supports Kogito'sÂ cloud-native runtime engine for creatingÂ rules, decisions, and resource-planning optimization solutions based on the &lt;a href="http://dmg.org/pmml/pmml-v4-4.html"&gt;Predictive Model Markup Language&lt;/a&gt; (PMML).&lt;/p&gt; &lt;p&gt;We'll present an example using &lt;a href="https://kogito.kie.org/"&gt;Kogito&lt;/a&gt; with &lt;a href="https://drools.org/"&gt;Drools Rules Language&lt;/a&gt;, both backed by the &lt;a href="http://kie.org/"&gt;KIE group&lt;/a&gt;. By expanding Kogito with the power of &lt;a href="https://quarkus.io/"&gt;Quarkus&lt;/a&gt;, you can enjoy hot-reload during the development phase and compile decision services into fast, lightweight services.&lt;/p&gt; &lt;p&gt;For resource planning, Process Automation Manager 7.11 brings full support for &lt;a href="https://www.optaplanner.org/download/releaseNotes/releaseNotes8.html"&gt;OptaPlanner 8&lt;/a&gt;, the most recent version of this &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;artificial intelligence&lt;/a&gt; (AI) constraint solver technology.&lt;/p&gt; &lt;p&gt;All these new features are now part of the &lt;a href="https://www.redhat.com/en/products/process-automation"&gt;Red Hat Process Automation&lt;/a&gt; stack.Â &lt;/p&gt; &lt;h2&gt;Decision services with Kogito&lt;/h2&gt; &lt;p&gt;Kogito is the open source project that brings a new cloud-native runtime engine to &lt;a href="https://developers.redhat.com/products/red-hat-decision-manager/"&gt;Red Hat Decision Manager&lt;/a&gt; and &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/process-automation-manager"&gt;Process Automation Manager&lt;/a&gt;. Kogito brings Drools and OptaPlanner's battle-tested capabilities with additional improvements for performance and usability. It provides a new set of tools that are better integrated with development IDEs such as &lt;a href="https://developers.redhat.com/products/vscode-extensions/overview"&gt;Visual Studio Code (VS Code)&lt;/a&gt; and with versioning tools such as GitHub that are already helping developers create decision services more efficiently.&lt;/p&gt; &lt;p&gt;The Kogito decision engine generates 90% of the code you need to get your service up and running. Kogito code-gen uses DMN nodes and data types to infer domain-driven APIs that can be exposed via REST along with the inputs and outputs of the respective APIs, based on the &lt;a href="https://swagger.io/"&gt;Swagger&lt;/a&gt; specification.&lt;/p&gt; &lt;h2&gt;Red Hat support for Kogito tools&lt;/h2&gt; &lt;p&gt;To support the development of decision services, the KIE team provides you with a good set of tooling to use in different spaces. As of today, Red Hat supports the following tools:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Business Central (in Process Automation Manager) or Decision Central (in Decision Manager): A business-friendly user interface.&lt;/li&gt; &lt;li&gt;&lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.vscode-extension-red-hat-business-automation-bundle"&gt;Business Automation VS Code Extension&lt;/a&gt;: A VS Code extension that allows the visualization and editing of &lt;a href="https://www.bpmn.org"&gt;Business Process Modeling Notation&lt;/a&gt; (BPMN), DMN, and test scenario files inside VS Code.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Other community-provided tools, free for use, are backed by Red Hat and the KIE team:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://learn-dmn-in-15-minutes.com/"&gt;Learn DMN in 15 minutes&lt;/a&gt;: A guided web-based tour through the elements of Decision Model and Notation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://chrome.google.com/webstore/detail/bpmn-dmn-test-scenario-ed/mgkfehibfkdpjkfjbikpchpcfimepckf"&gt;GitHub Chrome Extension&lt;/a&gt;: A browser extension that allows you to visualize and edit BPMN, DMN, and test scenario files directly in GitHub.&lt;/li&gt; &lt;li&gt;&lt;a href="https://kiegroup.github.io/kogito-online/#/"&gt;Online Editors&lt;/a&gt;: &lt;ul&gt;&lt;li&gt;&lt;a href="http://bpmn.new/"&gt;BPMN.new&lt;/a&gt;: A free online editor for business processes.&lt;/li&gt; &lt;li&gt;&lt;a href="http://dmn.new/"&gt;DMN.new&lt;/a&gt;: A free online editor for decision models.&lt;/li&gt; &lt;li&gt;&lt;a href="http://pmml.new/"&gt;PMML.new&lt;/a&gt;: A free online editor for scorecards.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://kiegroup.github.io/kogito-online/#/download"&gt;Business Modeler Hub&lt;/a&gt;: Allows downloads of the VS Code Extension, GitHub Chrome extension, and desktop app.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;With the &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.vscode-extension-red-hat-business-automation-bundle"&gt;VS Code Red Hat Business Automation extension&lt;/a&gt;, you can create new DMN diagrams by simply adding new files with the &lt;code&gt;.dmn&lt;/code&gt; extension.&lt;/p&gt; &lt;h2&gt;Kogito withÂ Decision Model and Notation&lt;/h2&gt; &lt;p&gt;Decision Model and Notation includes a TCK that allows vendors to check the conformity of their tooling. The &lt;a href="https://dmn-tck.github.io/tck/"&gt;DMN TCK&lt;/a&gt; test kit has three different levels of conformance, where Level 3Â means that the tested tool provides all the capabilities required by the specification. Currently, the Kogito runtime is compliant with version 1.3, the most recent version of DMN, in conformance with Level 3.&lt;/p&gt; &lt;p&gt;The first Kogito engine version supported by Red Hat is Kogito 1.5.x, which is supported under Red Hat Process Automation version 7.11.x. You can choose to run your decision services on top of the &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Red Hat build of Quarkus&lt;/a&gt; 1.11.x or &lt;a href="https://developers.redhat.com/topics/spring-boot"&gt;SpringBoot&lt;/a&gt; 2.3.4.&lt;/p&gt; &lt;h2&gt;Creating a decision service with Kogito and Quarkus&lt;/h2&gt; &lt;p&gt;To create your first decision service, use one of the available Kogito archetypes to generate the project for you. The following command generates a Quarkus-based decision service with the name &lt;code&gt;sample-kogito&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mvn archetype:generate \ -DarchetypeGroupId=org.kie.kogito \ -DarchetypeArtifactId=kogito-quarkus-archetype \ -DgroupId=org.acme -DartifactId=sample-kogito \ -DarchetypeVersion=1.5.0.Final \ -Dversion=1.0-SNAPSHOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you open the project in VS Code, you can already see a sample DMN created for you, as shown in Figure 1.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-05-26%20at%2009.58.15.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Screen%20Shot%202021-05-26%20at%2009.58.15.png?itok=vcMXAv1y" width="600" height="413" alt="When you create a project using Kogito, it provides a sample DMN." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Sample DMN created by Kogito. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Notice there are no Java classes or POJOs in this project. You can run this project and check the exposed APIs by running the following command in your terminal:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mvn quarkus:dev&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the project starts, you should see log messages similar to:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;2021-05-26 09:59:49,581 INFO [io.quarkus] (Quarkus Main Thread) sample-kogito 1.0-SNAPSHOT on JVM (powered by Quarkus 1.11.5.Final) started in 4.158s. Listening on: http://localhost:8080 2021-05-26 09:59:49,583 INFO [io.quarkus] (Quarkus Main Thread) Profile dev activated. Live Coding activated. 2021-05-26 09:59:49,583 INFO [io.quarkus] (Quarkus Main Thread) Installed features: [cdi, kogito-decisions, kogito-predictions, kogito-processes, kogito-rules, resteasy, resteasy-jackson, servlet, smallrye-health, smallrye-openapi, swagger-ui]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If the run is successful, you can access &lt;a href="http://http//:localhost:8080/swagger-ui"&gt;http//:localhost:8080/swagger-ui&lt;/a&gt; in your browser and check the existing APIs, as shown in Figure 2.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kogito-dmn-auto-generated-rest-apis.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/kogito-dmn-auto-generated-rest-apis.png?itok=opZtde4d" width="600" height="361" alt="Kogito auto-generates RESTful APIs for the DMN." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: RESTful APIs auto-generated by Kogito for DMN. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Creating and running cloud-native decision microservices is pretty straightforward when using Kogito. If you are interested in trying out Kogito decision services with DMN, here are some places you can go to get started:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Hands-on experience on Katacoda: &lt;a href="https://learn.openshift.com/middleware/courses/middleware-kogito/"&gt;Red Hat OpenShift's Interactive Learning Portal for Kogito&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Kogito Documentation: &lt;a href="https://docs.jboss.org/kogito/release/latest/html_single/#chap-kogito-using-drl-rules"&gt;Using DRL rules in Kogito services&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;If you are interested in how to run Drools Rules Language-based rules on Kogito, please refer to &lt;a href="https://developers.redhat.com/articles/2021/05/26/delivering-rule-based-services-kogito"&gt;this article&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The Kogito community is pretty active. You can always reach out to other community members and the Red Hat team behind it through the &lt;a href="http://kie.org/"&gt;community channels&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/17/how-deliver-decision-services-kogito" title="How to deliver decision services with Kogito"&gt;How to deliver decision services with Kogito&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/iMnvBPVvqYA" height="1" width="1" alt=""/&gt;</summary><dc:creator>Karina Varela</dc:creator><dc:date>2021-06-17T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/17/how-deliver-decision-services-kogito</feedburner:origLink></entry><entry><title type="html">Red Hat Summit 2021 (Ask the Experts) - An open approach to solution architectures (video)</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/YNeLYi8hXA4/red-hat-summit-2021-ask-the-experts-open-approach-solution-architectures-video.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/IThWqUU8sTQ/red-hat-summit-2021-ask-the-experts-open-approach-solution-architectures-video.html</id><updated>2021-06-17T05:00:00Z</updated><content type="html">This year theÂ Â event is a bit different as we bridge the gap from pandemic reality to hopefully a form of normalcy.Â  As the Red Hat Summit site explains to us, this "...event is expanding to become an all-new, flexible conference series, consisting of a 2âpart immersive virtual experience as well as a global tour of small-scale, in-person events. This series will create collective opportunities to share experiences, innovations, and insights." Â and the on-demand recording is available if you missed it.Â  The event is free, so if you have not yet done so, register and you have full access to all the recordings. Now let's take a look at how to jump straight to our session This year I'm involved with some friends and colleagues in a few Ask The Experts sessions where you get a live session on a topic with several experts available to ask anything you like about the topic. I've doneÂ Â in April, and I'm going to be involved in the following session next week: Solution architectures are the detailed and structured descriptions of the features, process, and behaviour of a solution. It acts as the base of the solution to define, deliver, manage and operate the development process of the solution. It identifies the alternatives of the solutions and its components. In this session, we'll highlight the process that Red Hat uses to compile and publish solution architectures for various business and technology scenarios that are based on actual use cases pertinent to our global customers and partners. It includes the key capabilities of continuous global collaboration with the engineering teams. Date: Tuesday, Jun 15 Time: 16:30 - 17:00 CEST Speakers:Â  * Eric D. Schabell, Portfolio Architect Technical Director, Red Hat * Will Nix, Senior Manager, Red Hat * E.G Nadhan, Chief Architect and Strategist, Red Hat I hope you enjoyed our discussions, the questions we had time to answer, and the information we shared around how we are developing our open architectures to share with you.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/YNeLYi8hXA4" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/IThWqUU8sTQ/red-hat-summit-2021-ask-the-experts-open-approach-solution-architectures-video.html</feedburner:origLink></entry></feed>
